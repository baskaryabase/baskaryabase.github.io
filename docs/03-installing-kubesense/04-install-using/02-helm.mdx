---
title: Install using Helm
sidebar_label: Helm
---

## Setting Context for Kubesense Deployment
Before deploying Kubesense in your on-premise Kubernetes environment,
it's important to understand the role of server and sensor components and how they interact to provide observability into your clusters.

### Overview of Kubesense Components
#### 1. Sensor Components:
  - The sensor is responsible for collecting data from the Kubernetes cluster.
  - It leverages eBPF to trace system and network activities, capturing crucial metrics such as pod communications, resource utilization, and latency.
  - This data is then sent to the Kubesense server for processing and storage.
  - Multiple sensors can be deployed across different clusters to monitor them individually.

#### 2. Server Components:
  - The server receives data from one or more sensors, processes the observability information, and stores it in a central database or data warehouse.
  - The server component includes the dashboard that allows users to visualize and analyze the data collected from sensors, providing insights into the clusterâ€™s behavior.
  - The server serves as the centralized management point for all the clusters where sensors are deployed.



## Add Helm repo
Add the official Kubesense Helm chart repository:
```
helm repo add kubesense https://helm.kubesense.ai
```
Update helm repositories
```
helm repo update
```

## Create access-token for kubesense
```
helm upgrade -i access-token -n kubesense kubesense/access-token --set KEY_ID="<KEY>" --set KEY_SECRET="SECRET"
```

## Deploy Kubesense for In-Cluster Deployment
Deploy Kubesense Server and Sensor together using Helm:
```bash
helm upgrade -i kubesense kubesense/kubesense \
            --create-namespace \
            -n kubesense \ 
            --set global.cluster_name="k8s-cluster" \
            --set global.dashboardHostName="<INGRESS_HOSTNAME_FOR_UI>"
```

- ### cluster_name
  This parameter sets the global configuration value for the cluster name.
  It allows you to uniquely identify the Kubernetes cluster where Kubesense is deployed. This is particularly useful in multi-cluster environments, where data from various clusters needs to be aggregated and displayed on a central dashboard. Each cluster should have a distinct name for clear identification.

- ### dashboardHostName
  This defines the external URL or hostname where the Kubesense dashboard can be accessed. It is typically set to the Ingress hostname or DNS that resolves to the UI service.

## Custom Deployment Using custom-values.yaml

When deploying KubeSense with Helm,
you have the flexibility to pass configuration values in two ways:
using the `--set` flag or by creating a custom `values.yaml` file.
While the `--set` flag is suitable for simple overrides, using a custom `values.yaml`
file is more efficient and manageable for complex deployments.
<details>
<summary>Deploy using a custom-values.yaml</summary>
### creating a custom-values.yaml file
```bash
cat << EOF > custom-values.yaml
global:
  cluster_name: k8s-cluster # cluster_name of your choice
  dashboardHostName: kubesense.example.com
  redis:
    password: custom_redis_password # {optional} if not specified will autogenerate
  clickhouse:
    password: custom_clickhouse_password # {optional} if not specified will autogenerate
  mysql:
    password: custom_mysql_password # {optional} if not specified will autogenerate
EOF
```
### Installation command using custom-values.yaml
```bash
helm upgrade -i kubesense kubesense/kubesense --create-namespace -n kubesense -f custom-values.yaml
```
</details>

## Deploying Server & Sensor Separately
## Deploy Server
```bash
helm upgrade -i kubesense kubesense/kubesense-server
            --create-namespace \
            -n kubesense \
            --set global.cluster_name="k8s-cluster" \
            --set global.dashboardHostName="kubesense.example.com"
```

## Deploy Sensor
When deploying the Kubesense Sensor in a Kubernetes cluster that is external to the Kubesense Server,
specific IP addresses need to be configured to ensure proper communication between the sensor and server. This is particularly important when using a NodePort service configuration for external access.

:::info
For more information on networking and firewall setup to ensure proper communication between the server and sensor,
you can refer to the detailed guide [here](/installing-kubesense/networking).
:::

### kubecolIps
The **kubecolIps** refers to the IP address or addresses of the Kubesense Server's `kubecol` endpoint.
This is where the sensor will send all its data for processing and storage.
Set this value to the public or NodePort IP of the server node where the Kubesense Server's collector service is exposed.
```bash
--set global.kubecolIps="{192.168.1.100}"
```
If the Kubesense Server collector is accessible via multiple IP addresses (e.g., in a highly available setup), you can specify a comma-separated list of IPs.
```bash
--set global.kubecolIps="{192.168.1.100,192.168.1.101}"
```

### vmAgentIp
The **vmAgentIp** refers to the IP address of the victoria metrics agent responsible for monitoring and collecting resource metrics.
The sensor needs to send the system-level data (e.g., CPU, memory usage) to this agent.
This should be the IP address where the VM agent
```bash
--set global.vmAgentIp="192.168.1.200"
```

### kubeAggregatorIp
The **kubeAggregatorIp** is the IP address of the Kubesense Aggregator,
which is responsible for combining and processing the observability data(logs & events) from multiple sensors.
The sensor sends the collected data to the aggregator, which then passes it on to the central server for storage.
```bash
--set global.kubeAggregatorIp="192.168.1.150"
```

When deploying the Kubesense Sensor to an external cluster,
it's essential to correctly configure the IP addresses of the server components to ensure proper communication.
The kubecolIps, vmAgentIp, and kubeAggregatorIp settings ensure that the sensor can send collected data to the appropriate server endpoints.
By specifying these IP addresses in your Helm deployment,
you ensure seamless integration and observability across clusters.

### Installing Sensor
```bash
helm upgrade -i kubesensor kubesense/kubesensor
            --create-namespace \
            -n kubesense \
            --set global.cluster_name="k8s-cluster" \
            --set global.dashboardHostName="kubesense.example.com" \
            --set global.kubecolIps="{192.168.1.100}" \
            --set global.vmAgentIp="192.168.1.200" \
            --set global.kubeAggregatorIp="192.168.1.150"
```

<details>
<summary>Deploy using a custom-values.yaml</summary>
### creating a custom-values.yaml file
```bash
cat << EOF > custom-values.yaml
global:
  cluster_name: k8s-cluster # cluster_name of your choice
  dashboardHostName: kubesense.example.com
  kubecolIps:
  - 10.0.0.1
  kubeAggregatorIp: <KUBEAGGREGATOR_IP> # eg. kubesense-kubeAggregator
  vmAgentIp: <KUBEAGGREGATOR_IP> # eg. kubesense-victoria-metrics-agent
  vmAgentHttpPort: 30060  
  nodePort: 
    kubecolCollector: 32133
    kubecolGrpc: 32033
    kubeAggregatorGrpcPort: 30051
    kubeAggregatorLogPort: 30052
EOF
```
### Installation command using custom-values.yaml
```bash
helm upgrade -i kubesense kubesense/kubesense --create-namespace -n kubesense -f custom-values.yaml
```
</details>