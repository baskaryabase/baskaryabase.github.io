---
title: Affinities & Tolerations
---

# üìå Affinities & Tolerations

**Affinities** and **Tolerations** are essential configurations in Kubernetes to control where pods are scheduled within the cluster. These settings help ensure that specific components of Kubesense are placed on appropriate nodes, allowing for optimized resource use, performance, and resiliency.

---

## üéØ Setting Node Affinity

Node affinity allows you to control pod placement based on specific labels assigned to nodes. In Kubesense, you can set a **global node affinity** or configure affinity for individual components.

### Global Node Affinity

To set node affinity for all Kubesense pods, use the `global.nodeAffinityLabelSelector` field in your configuration. This setting applies a hard label selector, ensuring that all Kubesense pods are scheduled only on nodes that meet the specified criteria.

```yaml
global:
  ...
  nodeAffinityLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values: kubesense
```

In this example, all pods are scheduled on nodes labeled with `app: kubesense`.

---

## üö¶ Adding Tolerations

Tolerations allow pods to be scheduled on nodes with specific taints. In Kubesense, global tolerations are set under `global.tolerations` and apply to all pods in the deployment.

### Global Tolerations

Set tolerations globally to allow all Kubesense pods to tolerate certain taints on nodes. This is useful for scheduling on nodes that may have restricted access or specific workload requirements.

```yaml
global:
  ...
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "kubesense"
      effect: "NoSchedule"
```

In this example, all pods are configured to tolerate nodes with the taint `app=kubesense:NoSchedule`, allowing them to be scheduled on such nodes.

---

## üîÑ Individual Affinities and Tolerations

In cases where you have horizontally scaled specific Kubesense components, or if certain components require node-specific configurations, you can apply **individual affinities and tolerations** for each component.

> **Note**: Setting affinities and tolerations per component ensures that specific workloads are scheduled optimally, based on node characteristics or specific requirements.

### Component-Specific Affinities and Tolerations

#### 1. ClickHouse

Configure node affinity and tolerations specifically for the **ClickHouse** component to place it on nodes labeled for ClickHouse workloads.

```yaml
clickhouse:
  ...
  nodeAffinityLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values: clickhouse_kubesense
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "clickhouse_kubesense"
      effect: "NoSchedule"
```

#### 2. Kubecol

Set node affinity and tolerations specifically for the **Kubecol** component, allowing it to be scheduled on nodes designated for Kubecol workloads.

```yaml
kubecol:
  ...
  nodeAffinityLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values: kubecol_kubesense
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "kubecol_kubesense"
      effect: "NoSchedule"
```

#### 3. Aggregator

Define specific affinity and toleration settings for the **Aggregator** component to ensure it runs on nodes optimized for data aggregation workloads.

```yaml
aggregator:
  ...
  nodeAffinityLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values: aggregator_kubesense
  tolerations:
    - key: "app"
      operator: "Equal"
      value: "aggregator_kubesense"
      effect: "NoSchedule"
```

---

## üìù Summary

- **Global Settings**: Use `global.nodeAffinityLabelSelector` and `global.tolerations` to apply affinity and toleration settings across all Kubesense pods.
- **Individual Settings**: Configure affinities and tolerations for specific components to optimize scheduling based on component requirements.

Setting up affinities and tolerations effectively allows Kubesense components to operate more efficiently, ensuring that pods are deployed on nodes best suited to their workloads. This setup is especially useful in larger clusters with diverse workloads and varying node specifications.